{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versions Python et Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "# import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modélisation CNN avec tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "# Gestion des images : lecture, transformations\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "\n",
    "# Gestion de l'architecture du réseau\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Conv2D, Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Resizing, Rescaling, BatchNormalization\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "\n",
    "# Architecture de modèles de réseaux pré-entrainés (fonctionnalité de Transfer Learning)\n",
    "from tensorflow.keras.applications import MobileNetV2, VGG16\n",
    "\n",
    "# Algorithme d'optimisation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# Sauvegarde, arret\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Open CV\n",
    "import cv2\n",
    "\n",
    "# Performances des modeles\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin des 2 dossiers d'images par classe à prédire pour l'apprentissage et l'évaluation des performances et l'inférence\n",
    "SRC_PATH_TRAIN = \"./dataset_footprint/train/\"\n",
    "SRC_PATH_TEST = \"./dataset_footprint/test/\"\n",
    "\n",
    "# Liste des catégories (classes à prévoir)\n",
    "LST_LABELS = os.listdir(SRC_PATH_TRAIN)\n",
    "\n",
    "# Parametres pour la generation d'images\n",
    "SEED_VALUE = 42\n",
    "VALID_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 160  # Taille de l'image IMG_SIZExIMG_SIZE (on augmente la resolution de 100 a 160)\n",
    "BATCH_SIZE = 10  # nb de données à passer pour un A/R dans le réseau (total de 77 images x 3 classes)\n",
    "NB_EPOCHS = 30  # Nb de passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Castor',\n",
       " 'Chat',\n",
       " 'Chien',\n",
       " 'Coyote',\n",
       " 'Ecureuil',\n",
       " 'Lapin',\n",
       " 'Loup',\n",
       " 'Lynx',\n",
       " 'Ours',\n",
       " 'Puma',\n",
       " 'Rat',\n",
       " 'RatonLaveur',\n",
       " 'Renard']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Définition comme label les noms des sous-dossiers de travail\n",
    "labels = os.listdir(SRC_PATH_TRAIN)\n",
    "LST_DIR_LABELS = labels\n",
    "LST_DIR_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle optimal (nom et sous-dossier)\n",
    "CKPT_NO, MDL_NAME = 'ckpt_footprints_1', '3footprints_CNN'\n",
    "CKPT_DIR = './'+ CKPT_NO\n",
    "PATH_BEST_MDL = CKPT_DIR + '/' + MDL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions Locales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(history):\n",
    "    \"\"\"\n",
    "    Fonction de tracé de la courbe d'ajustement d'un modèle\n",
    "    Arguments:\n",
    "        history : sequence de recueil des métriques d'évaluation d'un modèle lors de la phase d'apprentissage\n",
    "        les métriques sont ici prédéfinies : 'accuracy','val_accuracy','loss','val_loss'\n",
    "    Returns:\n",
    "        2 figures Matplotlib superposées des métriques 'accuracy' et 'loss' sur les datasets TRAIN et VALIDATION\n",
    "    \"\"\"\n",
    "    # Définition des séquences de sauvegarde des performances en TRAIN (accuracy et loss) et VALIDATION (val_)\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    # Liste des itérations de calcul : epochs\n",
    "    lst_epochs = range(len(accuracy))\n",
    "    \n",
    "    # Tracé en 2 figures\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(lst_epochs, accuracy, \"b\", label=\"accuracy [TRAIN]\")\n",
    "    plt.plot(lst_epochs, val_accuracy, \"r\", label=\"accuracy [VALIDATION]\")\n",
    "    plt.title(\"Exactitude du modèle\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(lst_epochs, loss, \"b\", label=\"loss [TRAIN]\")\n",
    "    plt.plot(lst_epochs, val_loss, \"r\", label=\"loss [VALIDATION]\")\n",
    "    plt.title(\"Courbe de perte du modèle\")    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING THIS FUNCTION IS BASICALLY UNUSED\n",
    "\n",
    "# create a confusion matrix to visually represent incorrectly classified images\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, out_path=\"\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in classes], columns=[i for i in classes])\n",
    "    plt.figure(figsize=(3,3))\n",
    "    ax = sns.heatmap(df_cm, annot=True, square=True, fmt=\"d\", linewidths=.2, cbar_kws={\"shrink\": 0.8})\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/confusion_matrix.png\")  # as in the plot_model_history, the matrix is saved in a file called \"model_name_confusion_matrix.png\"\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de l'architecture du réseau de neurones convolutifs (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    # Initialisation du réseau\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "    \n",
    "    # Blocs de Convolution\n",
    "    # model.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3), padding='same'))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Couches de classification\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #model.add(Dense(16, activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    \n",
    "    # Couche de sortie\n",
    "    model.add(Dense(len(LST_LABELS), activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'inférence avec le modele model de l'image n° numero du dossier src_path de la classe categorie\n",
    "def f_footprint_predict(categorie, numero, model, src_path=SRC_PATH_TEST, lst_labels=LST_LABELS):\n",
    "    # Image à classifier dans la catégorie courante\n",
    "    id_image = categorie + '_' + numero + '.jpg'\n",
    "    lb_image = src_path + '/' + categorie + '/' + id_image\n",
    "    print('Image :', lb_image)\n",
    "\n",
    "    # Lecture et normalisation de l'image\n",
    "    img = load_img(lb_image, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    img_array = img_to_array(img, dtype=np.uint8)\n",
    "    img_array = np.array(img_array)/255.0\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    # Affichage\n",
    "    img = cv2.imread(lb_image)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    print(plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "    plt.show()\n",
    "\n",
    "    # Prédiction et évaluation\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Cette image appartient probablement à la classe {} avec un niveau de confiance à {:.2f}.\"\n",
    "        .format(lst_labels[np.argmax(score)], 100 * np.max(score)))\n",
    "\n",
    "    print(tf.nn.softmax(predictions).numpy())\n",
    "    return predictions, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_trainable(model):\n",
    "    ''' Statut et nom des couches d'un modèle\n",
    "    '''\n",
    "    for layer in model.layers:\n",
    "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objet générateur de données de type images \n",
    "# Séquence de transformation à appliquer à la volée : normalisation (taille d'image), rotation, zoom, ...\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2, \n",
    "        horizontal_flip=True,    \n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,  # 0.05\n",
    "        height_shift_range=0.2,\n",
    "        #fill_mode=\"nearest\",\n",
    "        validation_split=0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 176 images belonging to 13 classes.\n",
      "{'Castor': 0, 'Chat': 1, 'Chien': 2, 'Coyote': 3, 'Ecureuil': 4, 'Lapin': 5, 'Loup': 6, 'Lynx': 7, 'Ours': 8, 'Puma': 9, 'Rat': 10, 'RatonLaveur': 11, 'Renard': 12} \n",
      "\n",
      "Found 37 images belonging to 13 classes.\n",
      "Found 52 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# générateur qui définit à la volée des données à partir du jeu de données sources\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=SRC_PATH_TRAIN,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',  # binaire <=> vecteurs de proba, sparse <=> index de la classe, categorical\n",
    "    #classes=LST_LABELS,\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=SEED_VALUE\n",
    ")\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "print(labels,'\\n')\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    directory=SRC_PATH_TRAIN,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',   # \"sparse\",\n",
    "    #classes=LST_LABELS,\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=SEED_VALUE\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=SRC_PATH_TEST,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    class_mode='sparse',   #None,\n",
    "    shuffle=False,\n",
    "    seed=SEED_VALUE  # pas utile ici\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du modèle et apprentissage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\Python Projects\\ia-wildlens\\epsiEnv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 300ms/step - accuracy: 0.0660 - loss: 2.6124 - val_accuracy: 0.1000 - val_loss: 2.5433\n",
      "Epoch 2/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 2.5594 - val_accuracy: 0.2857 - val_loss: 2.5445\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 229ms/step - accuracy: 0.1207 - loss: 2.5528 - val_accuracy: 0.1000 - val_loss: 2.5202\n",
      "Epoch 4/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3000 - loss: 2.5046 - val_accuracy: 0.2857 - val_loss: 2.5048\n",
      "Epoch 5/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 225ms/step - accuracy: 0.1308 - loss: 2.5399 - val_accuracy: 0.1333 - val_loss: 2.5071\n",
      "Epoch 6/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1000 - loss: 2.5316 - val_accuracy: 0.0000e+00 - val_loss: 2.5468\n",
      "Epoch 7/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.1445 - loss: 2.5239 - val_accuracy: 0.1333 - val_loss: 2.5423\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Instanciation du modèle CNN\n",
    "model = prepare_model()\n",
    "\n",
    "# Compilation du modèle\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Apprentissage sur le dataset TRAIN avec confrontation des performances avec le sous-ensemble de VALidation (20%)\n",
    "history = model.fit(train_generator, \n",
    "          validation_data=valid_generator,\n",
    "          verbose=1,\n",
    "          steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "          validation_steps=valid_generator.n//valid_generator.batch_size,\n",
    "          epochs=NB_EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          callbacks=[early_stopping]\n",
    "        #   callbacks=keras_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=./ckpt_footprints_1/3footprints_CNN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Configuration de keras_callbacks pour la sauvegarde du modèle optimal\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Configuration de l'arrêt anticipé par monitoring de la loss ou de l'accuracy (à expérimenter)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#early_stopping  = EarlyStopping(monitor='val_loss', patience=5, mode='auto', min_delta=0, verbose=1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m early_stopping  \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m keras_callbacks \u001b[38;5;241m=\u001b[39m [\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATH_BEST_MDL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     15\u001b[0m                    early_stopping]\n",
      "File \u001b[1;32mc:\\Users\\louis\\Python Projects\\ia-wildlens\\epsiEnv\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=./ckpt_footprints_1/3footprints_CNN"
     ]
    }
   ],
   "source": [
    "# Prepare a directory to store all the checkpoints.\n",
    "if not os.path.exists(CKPT_DIR):\n",
    "    os.makedirs(CKPT_DIR)\n",
    "\n",
    "# Configuration de keras_callbacks pour la sauvegarde du modèle optimal\n",
    "\n",
    "# Configuration de l'arrêt anticipé par monitoring de la loss ou de l'accuracy (à expérimenter)\n",
    "#early_stopping  = EarlyStopping(monitor='val_loss', patience=5, mode='auto', min_delta=0, verbose=1)\n",
    "early_stopping  = EarlyStopping(monitor='val_accuracy', patience=5, mode='auto', min_delta=0, verbose=1)\n",
    "\n",
    "keras_callbacks = [ModelCheckpoint(filepath=PATH_BEST_MDL, \n",
    "                                   monitor='val_accuracy', \n",
    "                                   save_best_only=True, \n",
    "                                   mode='auto'),\n",
    "                   early_stopping]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epsiEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
